{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18397145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch_geometric\n",
      "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
      "Collecting aiohttp (from torch_geometric)\n",
      "  Downloading aiohttp-3.11.14-cp311-cp311-win_amd64.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: fsspec in c:\\users\\chuny\\anaconda3\\envs\\comp4332\\lib\\site-packages (from torch_geometric) (2025.3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\chuny\\anaconda3\\envs\\comp4332\\lib\\site-packages (from torch_geometric) (3.1.6)\n",
      "Requirement already satisfied: numpy in c:\\users\\chuny\\anaconda3\\envs\\comp4332\\lib\\site-packages (from torch_geometric) (1.26.3)\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\chuny\\anaconda3\\envs\\comp4332\\lib\\site-packages (from torch_geometric) (5.9.0)\n",
      "Requirement already satisfied: pyparsing in c:\\users\\chuny\\anaconda3\\envs\\comp4332\\lib\\site-packages (from torch_geometric) (3.2.1)\n",
      "Requirement already satisfied: requests in c:\\users\\chuny\\anaconda3\\envs\\comp4332\\lib\\site-packages (from torch_geometric) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\chuny\\anaconda3\\envs\\comp4332\\lib\\site-packages (from torch_geometric) (4.67.1)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->torch_geometric)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->torch_geometric)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->torch_geometric)\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->torch_geometric)\n",
      "  Downloading frozenlist-1.5.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->torch_geometric)\n",
      "  Downloading multidict-6.2.0-cp311-cp311-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->torch_geometric)\n",
      "  Downloading propcache-0.3.1-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->torch_geometric)\n",
      "  Downloading yarl-1.18.3-cp311-cp311-win_amd64.whl.metadata (71 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\chuny\\anaconda3\\envs\\comp4332\\lib\\site-packages (from jinja2->torch_geometric) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\chuny\\anaconda3\\envs\\comp4332\\lib\\site-packages (from requests->torch_geometric) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chuny\\anaconda3\\envs\\comp4332\\lib\\site-packages (from requests->torch_geometric) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\chuny\\anaconda3\\envs\\comp4332\\lib\\site-packages (from requests->torch_geometric) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chuny\\anaconda3\\envs\\comp4332\\lib\\site-packages (from requests->torch_geometric) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\chuny\\anaconda3\\envs\\comp4332\\lib\\site-packages (from tqdm->torch_geometric) (0.4.6)\n",
      "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.1/1.1 MB 14.1 MB/s eta 0:00:00\n",
      "Downloading aiohttp-3.11.14-cp311-cp311-win_amd64.whl (443 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading frozenlist-1.5.0-cp311-cp311-win_amd64.whl (51 kB)\n",
      "Downloading multidict-6.2.0-cp311-cp311-win_amd64.whl (29 kB)\n",
      "Downloading propcache-0.3.1-cp311-cp311-win_amd64.whl (45 kB)\n",
      "Downloading yarl-1.18.3-cp311-cp311-win_amd64.whl (91 kB)\n",
      "Installing collected packages: propcache, multidict, frozenlist, attrs, aiohappyeyeballs, yarl, aiosignal, aiohttp, torch_geometric\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.14 aiosignal-1.3.2 attrs-25.3.0 frozenlist-1.5.0 multidict-6.2.0 propcache-0.3.1 torch_geometric-2.6.1 yarl-1.18.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install PyG if you haven't\n",
    "%pip install torch_geometric\n",
    "%pip install torch_sparse\n",
    "%pip install torch_scatter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ee1898",
   "metadata": {},
   "source": [
    "### Import PyG and PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdf637a",
   "metadata": {},
   "source": [
    "In this tutorial, we are going to introduce how to implement GraphSAGE to perform semi-supervised learning on a node classification task.\n",
    "\n",
    "- We will first demonstrate how to implement and train a GraphSAGE model for node classification without neighbor sampling.\n",
    "- Then, we will show how to use PyTorch Geometric's NeighborLoader to enable neighbor sampling for training and testing the GraphSAGE model.\n",
    "\n",
    "We will demonstrate this using the PyTorch Geometric package. However, feel free to explore other packages such as DGL.\n",
    "\n",
    "First, load PyTorch, PyTorch Geometric, and other necessary packages (we will also use NumPy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "419a8d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.loader import DataLoader, NeighborLoader\n",
    "from torch_geometric.nn import SAGEConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "967441a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65e3312",
   "metadata": {},
   "source": [
    "### Prepare the PubMed dataset\n",
    "We use a citation network called pubmed for demonstration. A node in the citation network is a paper and an edge represents the citation between two papers. \n",
    "\n",
    "This dataset has 19,717 papers and 88,648 citations. Each paper has a sparse bag-of-words feature vector and a class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cc17ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 19717\n",
      "Number of edges: 88648\n",
      "Number of features: 500\n",
      "Number of classes: 3\n",
      "Input feature size: 500\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the PubMed dataset\n",
    "dataset = Planetoid(root='/tmp/Pubmed', name='Pubmed')\n",
    "data = dataset[0]\n",
    "\n",
    "# Sparse bag-of-words features of papers\n",
    "features = data.x\n",
    "# Class labels of papers\n",
    "labels = data.y\n",
    "# Number of unique classes on the nodes\n",
    "n_classes = dataset.num_classes\n",
    "# input feature size\n",
    "in_feats = features.shape[1]\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Number of features: {features.shape[1]}')\n",
    "print(f'Number of classes: {n_classes}')\n",
    "print(f'Input feature size: {in_feats}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edf5de7",
   "metadata": {},
   "source": [
    "Here we remove all self-loops in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39a91a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[19717, 500], edge_index=[2, 88648], y=[19717], train_mask=[19717], val_mask=[19717], test_mask=[19717])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bf6755",
   "metadata": {},
   "source": [
    "### Implement the GNN model\n",
    "\n",
    "Essentially, given a graph structure, GNNs (GCN, GraphSAGE, GAT, etc.) are used to learn meaningful node representations (in this case, the embeddings, or vectors).\n",
    "Once these embeddings are properly learnt, we may perform downstream tasks such as node classification, graph classification, and link prediction.\n",
    "\n",
    "PyG provides two ways of implementing a GNN model:\n",
    "\n",
    "- using the nn module, which contains many commonly used GNN modules.\n",
    "- using the message passing interface to implement a GNN model from scratch.\n",
    "\n",
    "For simplicity, we implement the GraphSAGE model in the tutorial with the nn module.\n",
    "\n",
    "If you are interested in using the message passing interface to implement a GNN model, check this link https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html out.\n",
    "\n",
    "![fishy](https://raw.githubusercontent.com/dglai/WWW20-Hands-on-Tutorial/master/images/GNN.png)\n",
    "\n",
    "The GraphSage model has multiple layers. In each layer, a vertex accesses its direct neighbors. When we stack $k$ layers in a model, a node $v$ access neighbors within $k$ hops. The output of the GraphSage model is **node embeddings** that represent the nodes and all information in the k-hop neighborhood.\n",
    "\n",
    "If you want to learn about the details of the SageConv layer, look at its official documantation at https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.GraphSAGE.html  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3565e369",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGEModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 n_hidden,\n",
    "                 out_dim,\n",
    "                 n_layers,\n",
    "                 activation,\n",
    "                 dropout,\n",
    "                 aggregator_type):\n",
    "        super(GraphSAGEModel, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # Input layer\n",
    "        self.layers.append(SAGEConv(in_feats, n_hidden, aggr=aggregator_type))\n",
    "        # Hidden layers\n",
    "        for i in range(n_layers - 1):\n",
    "            self.layers.append(SAGEConv(n_hidden, n_hidden, aggr=aggregator_type))\n",
    "        # Output layer\n",
    "        self.layers.append(SAGEConv(n_hidden, out_dim, aggr=aggregator_type))\n",
    "\n",
    "        self.activation = activation\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = layer(x, edge_index)\n",
    "            if self.activation is not None:\n",
    "                x = self.activation(x)\n",
    "            x = nn.functional.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Output layer\n",
    "        x = self.layers[-1](x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b504e26e",
   "metadata": {},
   "source": [
    "### Node classification (semi-supervised)\n",
    "Let us perform node classification in a semi-supervised setting. In this setting, we have the entire graph structure and all node features. We only have labels on some of the nodes. We want to predict the labels on other nodes. Even though some of the nodes do not have labels, they connect with nodes with labels. Thus, we train the model with both labeled nodes and unlabeled nodes. Semi-supervised learning can usually improve performance.\n",
    "\n",
    "![semisupervised](https://raw.githubusercontent.com/dglai/WWW20-Hands-on-Tutorial/master/images/node_classify1.png)\n",
    "\n",
    "This dependency graph shows a better view of how labeled and unlabled nodes are used in the training. \n",
    "\n",
    "![dependency](https://raw.githubusercontent.com/dglai/WWW20-Hands-on-Tutorial/master/images/node_classify2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6c81b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "n_hidden = 64\n",
    "n_layers = 2\n",
    "dropout = 0.5\n",
    "aggregator_type = 'mean'\n",
    "\n",
    "gconv_model = GraphSAGEModel(in_feats,\n",
    "                             n_hidden,\n",
    "                             n_classes,\n",
    "                             n_layers,\n",
    "                             F.relu,\n",
    "                             dropout,\n",
    "                             aggregator_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0800fd28",
   "metadata": {},
   "source": [
    "Now we create the node classification model based on the GraphSage model. The GraphSage model takes a data (include edge_index and node features) as input and computes node embeddings as output. With node embeddings, we use a cross entropy loss to train the node classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c37985dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeClassification(nn.Module):\n",
    "    def __init__(self, gconv_model):\n",
    "        super(NodeClassification, self).__init__()\n",
    "        self.gconv_model = gconv_model\n",
    "        self.loss_fcn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, data):\n",
    "        labels, train_mask = data.y, data.train_mask\n",
    "        logits = self.gconv_model(data)\n",
    "        # Compute the loss using the training mask\n",
    "        return self.loss_fcn(logits[train_mask], labels[train_mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffeda9e",
   "metadata": {},
   "source": [
    "After defining a model for node classification, we define the evaluation, train and test function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89fd66d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NCEvaluate(model, data, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Compute embeddings with GNN\n",
    "        logits = model.gconv_model(data)\n",
    "        logits = logits[mask]\n",
    "        labels = data.y[mask]\n",
    "\n",
    "        # Get the predicted class indices\n",
    "        _, indices = torch.max(logits, dim=1)\n",
    "        correct = torch.sum(indices == labels)\n",
    "        acc = correct.item() * 1.0 / len(labels)\n",
    "    return acc\n",
    "\n",
    "def Train(model, data, optimizer, n_epochs):\n",
    "    for epoch in range(n_epochs):\n",
    "        # Set the model in training mode\n",
    "        model.train()\n",
    "\n",
    "        # Forward pass and compute loss\n",
    "        loss = model(data)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Evaluate on the validation set\n",
    "        val_acc = NCEvaluate(model, data, data.val_mask)\n",
    "        print(\"Epoch {:05d} | Loss {:.4f} | Validation Accuracy {:.4f}\".format(epoch, loss.item(), val_acc))\n",
    "\n",
    "def Test(model, data):\n",
    "    test_acc = NCEvaluate(model, data, data.test_mask)\n",
    "    print('Testing Accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f892a9f",
   "metadata": {},
   "source": [
    "Prepare data for semi-supervised node classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10a803e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Data statistics------'\n",
      "      #Classes 3\n",
      "      #Train samples 60\n",
      "      #Val samples 500\n",
      "      #Test samples 1000\n"
     ]
    }
   ],
   "source": [
    "train_mask = data.train_mask\n",
    "val_mask = data.val_mask\n",
    "test_mask = data.test_mask\n",
    "\n",
    "print(\"\"\"----Data statistics------'\n",
    "      #Classes {}\n",
    "      #Train samples {}\n",
    "      #Val samples {}\n",
    "      #Test samples {}\"\"\".format(\n",
    "          data.y.max().item() + 1,  # Assuming y contains class labels starting from 0\n",
    "          train_mask.sum().item(),\n",
    "          val_mask.sum().item(),\n",
    "          test_mask.sum().item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9395759",
   "metadata": {},
   "source": [
    "After defining the model and evaluation function, we can put everything into the training loop to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36b9ebbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Loss 1.1010 | Validation Accuracy 0.1960\n",
      "Epoch 00001 | Loss 1.0986 | Validation Accuracy 0.1960\n",
      "Epoch 00002 | Loss 1.0977 | Validation Accuracy 0.1960\n",
      "Epoch 00003 | Loss 1.0981 | Validation Accuracy 0.1960\n",
      "Epoch 00004 | Loss 1.0919 | Validation Accuracy 0.1960\n",
      "Epoch 00005 | Loss 1.0963 | Validation Accuracy 0.1960\n",
      "Epoch 00006 | Loss 1.0928 | Validation Accuracy 0.1960\n",
      "Epoch 00007 | Loss 1.0897 | Validation Accuracy 0.1960\n",
      "Epoch 00008 | Loss 1.0764 | Validation Accuracy 0.1960\n",
      "Epoch 00009 | Loss 1.0846 | Validation Accuracy 0.1960\n",
      "Epoch 00010 | Loss 1.0846 | Validation Accuracy 0.2280\n",
      "Epoch 00011 | Loss 1.0771 | Validation Accuracy 0.3500\n",
      "Epoch 00012 | Loss 1.0755 | Validation Accuracy 0.4840\n",
      "Epoch 00013 | Loss 1.0795 | Validation Accuracy 0.5720\n",
      "Epoch 00014 | Loss 1.0652 | Validation Accuracy 0.6260\n",
      "Epoch 00015 | Loss 1.0644 | Validation Accuracy 0.6640\n",
      "Epoch 00016 | Loss 1.0621 | Validation Accuracy 0.6760\n",
      "Epoch 00017 | Loss 1.0636 | Validation Accuracy 0.7000\n",
      "Epoch 00018 | Loss 1.0518 | Validation Accuracy 0.7180\n",
      "Epoch 00019 | Loss 1.0488 | Validation Accuracy 0.7260\n",
      "Epoch 00020 | Loss 1.0480 | Validation Accuracy 0.7220\n",
      "Epoch 00021 | Loss 1.0314 | Validation Accuracy 0.7300\n",
      "Epoch 00022 | Loss 1.0267 | Validation Accuracy 0.7320\n",
      "Epoch 00023 | Loss 1.0216 | Validation Accuracy 0.7300\n",
      "Epoch 00024 | Loss 1.0094 | Validation Accuracy 0.7300\n",
      "Epoch 00025 | Loss 1.0122 | Validation Accuracy 0.7240\n",
      "Epoch 00026 | Loss 1.0003 | Validation Accuracy 0.7300\n",
      "Epoch 00027 | Loss 0.9785 | Validation Accuracy 0.7240\n",
      "Epoch 00028 | Loss 0.9695 | Validation Accuracy 0.7240\n",
      "Epoch 00029 | Loss 0.9614 | Validation Accuracy 0.7240\n",
      "Epoch 00030 | Loss 0.9597 | Validation Accuracy 0.7280\n",
      "Epoch 00031 | Loss 0.9427 | Validation Accuracy 0.7380\n",
      "Epoch 00032 | Loss 0.9358 | Validation Accuracy 0.7400\n",
      "Epoch 00033 | Loss 0.9226 | Validation Accuracy 0.7400\n",
      "Epoch 00034 | Loss 0.9167 | Validation Accuracy 0.7420\n",
      "Epoch 00035 | Loss 0.9107 | Validation Accuracy 0.7420\n",
      "Epoch 00036 | Loss 0.8955 | Validation Accuracy 0.7440\n",
      "Epoch 00037 | Loss 0.8755 | Validation Accuracy 0.7480\n",
      "Epoch 00038 | Loss 0.8654 | Validation Accuracy 0.7440\n",
      "Epoch 00039 | Loss 0.8500 | Validation Accuracy 0.7440\n",
      "Epoch 00040 | Loss 0.8180 | Validation Accuracy 0.7460\n",
      "Epoch 00041 | Loss 0.8046 | Validation Accuracy 0.7460\n",
      "Epoch 00042 | Loss 0.7743 | Validation Accuracy 0.7420\n",
      "Epoch 00043 | Loss 0.7791 | Validation Accuracy 0.7440\n",
      "Epoch 00044 | Loss 0.7454 | Validation Accuracy 0.7420\n",
      "Epoch 00045 | Loss 0.6820 | Validation Accuracy 0.7440\n",
      "Epoch 00046 | Loss 0.6979 | Validation Accuracy 0.7520\n",
      "Epoch 00047 | Loss 0.6750 | Validation Accuracy 0.7560\n",
      "Epoch 00048 | Loss 0.6624 | Validation Accuracy 0.7540\n",
      "Epoch 00049 | Loss 0.6298 | Validation Accuracy 0.7560\n",
      "Epoch 00050 | Loss 0.5718 | Validation Accuracy 0.7560\n",
      "Epoch 00051 | Loss 0.5999 | Validation Accuracy 0.7580\n",
      "Epoch 00052 | Loss 0.5584 | Validation Accuracy 0.7580\n",
      "Epoch 00053 | Loss 0.5158 | Validation Accuracy 0.7580\n",
      "Epoch 00054 | Loss 0.5091 | Validation Accuracy 0.7620\n",
      "Epoch 00055 | Loss 0.5021 | Validation Accuracy 0.7620\n",
      "Epoch 00056 | Loss 0.4178 | Validation Accuracy 0.7600\n",
      "Epoch 00057 | Loss 0.4316 | Validation Accuracy 0.7600\n",
      "Epoch 00058 | Loss 0.4303 | Validation Accuracy 0.7620\n",
      "Epoch 00059 | Loss 0.3897 | Validation Accuracy 0.7600\n",
      "Epoch 00060 | Loss 0.3634 | Validation Accuracy 0.7580\n",
      "Epoch 00061 | Loss 0.3616 | Validation Accuracy 0.7600\n",
      "Epoch 00062 | Loss 0.2985 | Validation Accuracy 0.7620\n",
      "Epoch 00063 | Loss 0.3021 | Validation Accuracy 0.7640\n",
      "Epoch 00064 | Loss 0.3017 | Validation Accuracy 0.7640\n",
      "Epoch 00065 | Loss 0.2822 | Validation Accuracy 0.7660\n",
      "Epoch 00066 | Loss 0.2522 | Validation Accuracy 0.7660\n",
      "Epoch 00067 | Loss 0.2446 | Validation Accuracy 0.7680\n",
      "Epoch 00068 | Loss 0.2099 | Validation Accuracy 0.7660\n",
      "Epoch 00069 | Loss 0.2248 | Validation Accuracy 0.7660\n",
      "Epoch 00070 | Loss 0.1959 | Validation Accuracy 0.7680\n",
      "Epoch 00071 | Loss 0.1934 | Validation Accuracy 0.7720\n",
      "Epoch 00072 | Loss 0.1926 | Validation Accuracy 0.7720\n",
      "Epoch 00073 | Loss 0.1747 | Validation Accuracy 0.7740\n",
      "Epoch 00074 | Loss 0.1641 | Validation Accuracy 0.7700\n",
      "Epoch 00075 | Loss 0.1745 | Validation Accuracy 0.7740\n",
      "Epoch 00076 | Loss 0.1576 | Validation Accuracy 0.7760\n",
      "Epoch 00077 | Loss 0.1149 | Validation Accuracy 0.7740\n",
      "Epoch 00078 | Loss 0.1124 | Validation Accuracy 0.7780\n",
      "Epoch 00079 | Loss 0.1159 | Validation Accuracy 0.7800\n",
      "Epoch 00080 | Loss 0.0966 | Validation Accuracy 0.7840\n",
      "Epoch 00081 | Loss 0.1287 | Validation Accuracy 0.7800\n",
      "Epoch 00082 | Loss 0.1047 | Validation Accuracy 0.7780\n",
      "Epoch 00083 | Loss 0.0764 | Validation Accuracy 0.7820\n",
      "Epoch 00084 | Loss 0.0793 | Validation Accuracy 0.7820\n",
      "Epoch 00085 | Loss 0.0657 | Validation Accuracy 0.7820\n",
      "Epoch 00086 | Loss 0.0978 | Validation Accuracy 0.7820\n",
      "Epoch 00087 | Loss 0.0683 | Validation Accuracy 0.7820\n",
      "Epoch 00088 | Loss 0.0625 | Validation Accuracy 0.7840\n",
      "Epoch 00089 | Loss 0.0514 | Validation Accuracy 0.7860\n",
      "Epoch 00090 | Loss 0.0477 | Validation Accuracy 0.7860\n",
      "Epoch 00091 | Loss 0.0472 | Validation Accuracy 0.7820\n",
      "Epoch 00092 | Loss 0.0378 | Validation Accuracy 0.7820\n",
      "Epoch 00093 | Loss 0.0411 | Validation Accuracy 0.7820\n",
      "Epoch 00094 | Loss 0.0478 | Validation Accuracy 0.7860\n",
      "Epoch 00095 | Loss 0.0362 | Validation Accuracy 0.7840\n",
      "Epoch 00096 | Loss 0.0388 | Validation Accuracy 0.7800\n",
      "Epoch 00097 | Loss 0.0405 | Validation Accuracy 0.7800\n",
      "Epoch 00098 | Loss 0.0394 | Validation Accuracy 0.7820\n",
      "Epoch 00099 | Loss 0.0258 | Validation Accuracy 0.7820\n",
      "Epoch 00100 | Loss 0.0285 | Validation Accuracy 0.7820\n",
      "Epoch 00101 | Loss 0.0325 | Validation Accuracy 0.7820\n",
      "Epoch 00102 | Loss 0.0288 | Validation Accuracy 0.7820\n",
      "Epoch 00103 | Loss 0.0467 | Validation Accuracy 0.7800\n",
      "Epoch 00104 | Loss 0.0289 | Validation Accuracy 0.7840\n",
      "Epoch 00105 | Loss 0.0425 | Validation Accuracy 0.7840\n",
      "Epoch 00106 | Loss 0.0320 | Validation Accuracy 0.7820\n",
      "Epoch 00107 | Loss 0.0227 | Validation Accuracy 0.7840\n",
      "Epoch 00108 | Loss 0.0242 | Validation Accuracy 0.7840\n",
      "Epoch 00109 | Loss 0.0299 | Validation Accuracy 0.7840\n",
      "Epoch 00110 | Loss 0.0258 | Validation Accuracy 0.7820\n",
      "Epoch 00111 | Loss 0.0203 | Validation Accuracy 0.7820\n",
      "Epoch 00112 | Loss 0.0227 | Validation Accuracy 0.7900\n",
      "Epoch 00113 | Loss 0.0142 | Validation Accuracy 0.7900\n",
      "Epoch 00114 | Loss 0.0213 | Validation Accuracy 0.7900\n",
      "Epoch 00115 | Loss 0.0215 | Validation Accuracy 0.7880\n",
      "Epoch 00116 | Loss 0.0152 | Validation Accuracy 0.7880\n",
      "Epoch 00117 | Loss 0.0221 | Validation Accuracy 0.7880\n",
      "Epoch 00118 | Loss 0.0136 | Validation Accuracy 0.7860\n",
      "Epoch 00119 | Loss 0.0205 | Validation Accuracy 0.7860\n",
      "Epoch 00120 | Loss 0.0158 | Validation Accuracy 0.7880\n",
      "Epoch 00121 | Loss 0.0214 | Validation Accuracy 0.7880\n",
      "Epoch 00122 | Loss 0.0180 | Validation Accuracy 0.7900\n",
      "Epoch 00123 | Loss 0.0138 | Validation Accuracy 0.7880\n",
      "Epoch 00124 | Loss 0.0138 | Validation Accuracy 0.7840\n",
      "Epoch 00125 | Loss 0.0277 | Validation Accuracy 0.7840\n",
      "Epoch 00126 | Loss 0.0212 | Validation Accuracy 0.7860\n",
      "Epoch 00127 | Loss 0.0114 | Validation Accuracy 0.7860\n",
      "Epoch 00128 | Loss 0.0213 | Validation Accuracy 0.7840\n",
      "Epoch 00129 | Loss 0.0183 | Validation Accuracy 0.7840\n",
      "Epoch 00130 | Loss 0.0145 | Validation Accuracy 0.7860\n",
      "Epoch 00131 | Loss 0.0122 | Validation Accuracy 0.7860\n",
      "Epoch 00132 | Loss 0.0135 | Validation Accuracy 0.7860\n",
      "Epoch 00133 | Loss 0.0156 | Validation Accuracy 0.7880\n",
      "Epoch 00134 | Loss 0.0182 | Validation Accuracy 0.7880\n",
      "Epoch 00135 | Loss 0.0218 | Validation Accuracy 0.7860\n",
      "Epoch 00136 | Loss 0.0146 | Validation Accuracy 0.7920\n",
      "Epoch 00137 | Loss 0.0113 | Validation Accuracy 0.7960\n",
      "Epoch 00138 | Loss 0.0138 | Validation Accuracy 0.7900\n",
      "Epoch 00139 | Loss 0.0096 | Validation Accuracy 0.7840\n",
      "Epoch 00140 | Loss 0.0190 | Validation Accuracy 0.7840\n",
      "Epoch 00141 | Loss 0.0096 | Validation Accuracy 0.7860\n",
      "Epoch 00142 | Loss 0.0115 | Validation Accuracy 0.7900\n",
      "Epoch 00143 | Loss 0.0092 | Validation Accuracy 0.7900\n",
      "Epoch 00144 | Loss 0.0180 | Validation Accuracy 0.7900\n",
      "Epoch 00145 | Loss 0.0083 | Validation Accuracy 0.7900\n",
      "Epoch 00146 | Loss 0.0068 | Validation Accuracy 0.7900\n",
      "Epoch 00147 | Loss 0.0108 | Validation Accuracy 0.7900\n",
      "Epoch 00148 | Loss 0.0101 | Validation Accuracy 0.7900\n",
      "Epoch 00149 | Loss 0.0089 | Validation Accuracy 0.7900\n",
      "Testing Accuracy: 0.755\n"
     ]
    }
   ],
   "source": [
    "# Node classification task\n",
    "model = NodeClassification(gconv_model)\n",
    "\n",
    "# Training hyperparameters\n",
    "weight_decay = 5e-4\n",
    "n_epochs = 150\n",
    "lr = 1e-3\n",
    "\n",
    "# create the Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "Train(model, data, optimizer, n_epochs)\n",
    "Test(model, data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc102d2f",
   "metadata": {},
   "source": [
    "The above example runs without neighbor sampling. Now, let's look at how to implement this feature in PyTorch Geometric (PyG).\n",
    "\n",
    "PyG provides support for neighbor sampling through the following utilities:\n",
    "\n",
    "`torch_geometric.loader.NeighborSampler`\n",
    "`torch_geometric.loader.DataLoader`\n",
    "Note that the GraphSAGE structure does not change; it is only a change in the training approach:\n",
    "\n",
    "We switch to batched training.\n",
    "Each node within a batch is updated with a portion of randomly sampled neighbors instead of all its neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87d6be25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_sparse\n",
    "import torch_scatter\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "\n",
    "# Define the batch size and fan-out\n",
    "batch_size = 1024\n",
    "fan_out = [10, 20, 30]  # Maximum number of neighbors in each layer\n",
    "\n",
    "def get_dataloader_with_sampling(data, mask, batch_size=32, shuffle=False):\n",
    "    # Create a NeighborSampler\n",
    "    sampler = NeighborLoader(data, input_nodes = mask, num_neighbors=fan_out, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    # Return the sampler and the node IDs|\n",
    "    return sampler\n",
    "\n",
    "# Assuming your data object has the necessary attributes (edge_index, train_mask, val_mask, test_mask)\n",
    "train_sampler = get_dataloader_with_sampling(data, data.train_mask, batch_size, shuffle=True)\n",
    "val_sampler = get_dataloader_with_sampling(data, data.val_mask, batch_size, shuffle=False)\n",
    "test_sampler = get_dataloader_with_sampling(data, data.test_mask, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368425f3",
   "metadata": {},
   "source": [
    "The model structure remains the same. \n",
    "And the only difference is at the **forward** function, where we adapt the function to receive `blocks` data as inputs, which are the batched neighborhood-sampled graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7c3521e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            if i < len(self.convs) - 1:\n",
    "                x = x.relu_()\n",
    "                x = F.dropout(x, p=0.5, training=self.training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1a1daa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeClassification(nn.Module):\n",
    "    def __init__(self, gconv_model):\n",
    "        super(NodeClassification, self).__init__()\n",
    "        self.gconv_model = gconv_model\n",
    "        self.loss_fcn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        logits = self.gconv_model(x, edge_index)\n",
    "        # Compute the loss using the training mask\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e9c6fe",
   "metadata": {},
   "source": [
    "For the training and evaluation, we re-organize them to receive batch input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7e73ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train(model, train_dataloader, val_dataloader, optimizer, n_epochs):\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        total_loss = total_correct = 0\n",
    "        for batch in train_dataloader:\n",
    "            y = batch.y[:batch.batch_size]\n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch.x, batch.edge_index)[:batch.batch_size]\n",
    "            loss = F.cross_entropy(out, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += float(loss)\n",
    "            total_correct += int(out.argmax(dim=-1).eq(y).sum())\n",
    "        loss = total_loss / len(train_dataloader)\n",
    "        approx_acc = total_correct / int(data.train_mask.sum())\n",
    "        val_acc = Evaluate(model, val_dataloader)\n",
    "        print(f\"Epoch {epoch:05d} | Loss {loss:.4f} | Accuracy {approx_acc:.4f} | Val Accuracy {val_acc:.4f}\")\n",
    "\n",
    "def Evaluate(model, eval_dataloader):\n",
    "    model.eval()\n",
    "    all_labels, all_logits = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_dataloader:\n",
    "            y = batch.y[:batch.batch_size]\n",
    "            out = model(batch.x, batch.edge_index)[:batch.batch_size]\n",
    "            all_logits.append(out)\n",
    "            all_labels.append(y)\n",
    "        labels = torch.cat(all_labels)\n",
    "        logits = torch.cat(all_logits)\n",
    "        acc = (logits.argmax(1) == labels).float().mean().item()\n",
    "    return acc\n",
    "\n",
    "def Test(model, eval_dataloader):\n",
    "    test_acc = Evaluate(model, eval_dataloader)\n",
    "    print('Testing Accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052391a2",
   "metadata": {},
   "source": [
    "Let's try training a model in this way..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca1ceae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "n_hidden = 64\n",
    "n_layers = 2\n",
    "dropout = 0.5\n",
    "aggregator_type = 'mean' \n",
    "gconv_model = SAGE(in_feats,\n",
    "                    n_hidden,\n",
    "                    n_classes)\n",
    "\n",
    "# Node classification task\n",
    "model = NodeClassification(gconv_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be92fede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Loss 1.1022 | Accuracy 0.3333 | Val Accuracy 0.4160\n",
      "Epoch 00001 | Loss 1.0973 | Accuracy 0.3333 | Val Accuracy 0.4160\n",
      "Epoch 00002 | Loss 1.0932 | Accuracy 0.3333 | Val Accuracy 0.4160\n",
      "Epoch 00003 | Loss 1.0887 | Accuracy 0.3333 | Val Accuracy 0.4160\n",
      "Epoch 00004 | Loss 1.0849 | Accuracy 0.3333 | Val Accuracy 0.4160\n",
      "Epoch 00005 | Loss 1.0804 | Accuracy 0.3333 | Val Accuracy 0.4160\n",
      "Epoch 00006 | Loss 1.0784 | Accuracy 0.3333 | Val Accuracy 0.4160\n",
      "Epoch 00007 | Loss 1.0699 | Accuracy 0.3667 | Val Accuracy 0.4160\n",
      "Epoch 00008 | Loss 1.0670 | Accuracy 0.3500 | Val Accuracy 0.4180\n",
      "Epoch 00009 | Loss 1.0642 | Accuracy 0.3833 | Val Accuracy 0.4300\n",
      "Epoch 00010 | Loss 1.0579 | Accuracy 0.4333 | Val Accuracy 0.4520\n",
      "Epoch 00011 | Loss 1.0501 | Accuracy 0.4833 | Val Accuracy 0.4800\n",
      "Epoch 00012 | Loss 1.0464 | Accuracy 0.5667 | Val Accuracy 0.5040\n",
      "Epoch 00013 | Loss 1.0412 | Accuracy 0.6333 | Val Accuracy 0.5120\n",
      "Epoch 00014 | Loss 1.0314 | Accuracy 0.6833 | Val Accuracy 0.5240\n",
      "Epoch 00015 | Loss 1.0258 | Accuracy 0.7500 | Val Accuracy 0.5320\n",
      "Epoch 00016 | Loss 1.0231 | Accuracy 0.7333 | Val Accuracy 0.5420\n",
      "Epoch 00017 | Loss 1.0102 | Accuracy 0.8333 | Val Accuracy 0.5520\n",
      "Epoch 00018 | Loss 1.0132 | Accuracy 0.7667 | Val Accuracy 0.5600\n",
      "Epoch 00019 | Loss 1.0046 | Accuracy 0.8167 | Val Accuracy 0.5680\n",
      "Epoch 00020 | Loss 0.9933 | Accuracy 0.8667 | Val Accuracy 0.5820\n",
      "Epoch 00021 | Loss 0.9915 | Accuracy 0.8667 | Val Accuracy 0.6080\n",
      "Epoch 00022 | Loss 0.9809 | Accuracy 0.9000 | Val Accuracy 0.6320\n",
      "Epoch 00023 | Loss 0.9675 | Accuracy 0.9000 | Val Accuracy 0.6480\n",
      "Epoch 00024 | Loss 0.9639 | Accuracy 0.9167 | Val Accuracy 0.6660\n",
      "Epoch 00025 | Loss 0.9554 | Accuracy 0.9500 | Val Accuracy 0.6760\n",
      "Epoch 00026 | Loss 0.9512 | Accuracy 0.9500 | Val Accuracy 0.6960\n",
      "Epoch 00027 | Loss 0.9382 | Accuracy 0.9500 | Val Accuracy 0.6980\n",
      "Epoch 00028 | Loss 0.9369 | Accuracy 0.9167 | Val Accuracy 0.7140\n",
      "Epoch 00029 | Loss 0.9207 | Accuracy 0.9667 | Val Accuracy 0.7240\n",
      "Epoch 00030 | Loss 0.9074 | Accuracy 0.9667 | Val Accuracy 0.7260\n",
      "Epoch 00031 | Loss 0.8999 | Accuracy 0.9667 | Val Accuracy 0.7240\n",
      "Epoch 00032 | Loss 0.8997 | Accuracy 0.9500 | Val Accuracy 0.7360\n",
      "Epoch 00033 | Loss 0.8923 | Accuracy 0.9667 | Val Accuracy 0.7300\n",
      "Epoch 00034 | Loss 0.8829 | Accuracy 0.9667 | Val Accuracy 0.7280\n",
      "Epoch 00035 | Loss 0.8696 | Accuracy 0.9333 | Val Accuracy 0.7360\n",
      "Epoch 00036 | Loss 0.8586 | Accuracy 0.9667 | Val Accuracy 0.7380\n",
      "Epoch 00037 | Loss 0.8527 | Accuracy 0.9667 | Val Accuracy 0.7360\n",
      "Epoch 00038 | Loss 0.8411 | Accuracy 0.9667 | Val Accuracy 0.7380\n",
      "Epoch 00039 | Loss 0.8292 | Accuracy 0.9500 | Val Accuracy 0.7340\n",
      "Epoch 00040 | Loss 0.8159 | Accuracy 0.9500 | Val Accuracy 0.7320\n",
      "Epoch 00041 | Loss 0.8062 | Accuracy 0.9333 | Val Accuracy 0.7380\n",
      "Epoch 00042 | Loss 0.7959 | Accuracy 0.9667 | Val Accuracy 0.7360\n",
      "Epoch 00043 | Loss 0.7886 | Accuracy 0.9333 | Val Accuracy 0.7380\n",
      "Epoch 00044 | Loss 0.7751 | Accuracy 0.9500 | Val Accuracy 0.7400\n",
      "Epoch 00045 | Loss 0.7579 | Accuracy 0.9500 | Val Accuracy 0.7420\n",
      "Epoch 00046 | Loss 0.7373 | Accuracy 0.9667 | Val Accuracy 0.7400\n",
      "Epoch 00047 | Loss 0.7339 | Accuracy 0.9667 | Val Accuracy 0.7440\n",
      "Epoch 00048 | Loss 0.7397 | Accuracy 0.9500 | Val Accuracy 0.7340\n",
      "Epoch 00049 | Loss 0.7188 | Accuracy 0.9667 | Val Accuracy 0.7380\n",
      "Epoch 00050 | Loss 0.7038 | Accuracy 0.9667 | Val Accuracy 0.7340\n",
      "Epoch 00051 | Loss 0.6952 | Accuracy 0.9833 | Val Accuracy 0.7360\n",
      "Epoch 00052 | Loss 0.6939 | Accuracy 0.9667 | Val Accuracy 0.7380\n",
      "Epoch 00053 | Loss 0.6681 | Accuracy 0.9667 | Val Accuracy 0.7380\n",
      "Epoch 00054 | Loss 0.6573 | Accuracy 0.9833 | Val Accuracy 0.7360\n",
      "Epoch 00055 | Loss 0.6506 | Accuracy 0.9667 | Val Accuracy 0.7380\n",
      "Epoch 00056 | Loss 0.6486 | Accuracy 0.9667 | Val Accuracy 0.7420\n",
      "Epoch 00057 | Loss 0.6099 | Accuracy 0.9833 | Val Accuracy 0.7400\n",
      "Epoch 00058 | Loss 0.6123 | Accuracy 0.9667 | Val Accuracy 0.7420\n",
      "Epoch 00059 | Loss 0.6104 | Accuracy 0.9667 | Val Accuracy 0.7400\n",
      "Epoch 00060 | Loss 0.5976 | Accuracy 0.9667 | Val Accuracy 0.7340\n",
      "Epoch 00061 | Loss 0.5791 | Accuracy 0.9500 | Val Accuracy 0.7420\n",
      "Epoch 00062 | Loss 0.5711 | Accuracy 0.9500 | Val Accuracy 0.7400\n",
      "Epoch 00063 | Loss 0.5462 | Accuracy 0.9667 | Val Accuracy 0.7440\n",
      "Epoch 00064 | Loss 0.5475 | Accuracy 0.9833 | Val Accuracy 0.7480\n",
      "Epoch 00065 | Loss 0.5288 | Accuracy 0.9667 | Val Accuracy 0.7480\n",
      "Epoch 00066 | Loss 0.5307 | Accuracy 0.9833 | Val Accuracy 0.7460\n",
      "Epoch 00067 | Loss 0.5070 | Accuracy 0.9667 | Val Accuracy 0.7480\n",
      "Epoch 00068 | Loss 0.5059 | Accuracy 0.9667 | Val Accuracy 0.7540\n",
      "Epoch 00069 | Loss 0.4873 | Accuracy 0.9667 | Val Accuracy 0.7500\n",
      "Epoch 00070 | Loss 0.5005 | Accuracy 0.9500 | Val Accuracy 0.7520\n",
      "Epoch 00071 | Loss 0.4517 | Accuracy 0.9833 | Val Accuracy 0.7540\n",
      "Epoch 00072 | Loss 0.4667 | Accuracy 0.9667 | Val Accuracy 0.7540\n",
      "Epoch 00073 | Loss 0.4716 | Accuracy 0.9667 | Val Accuracy 0.7540\n",
      "Epoch 00074 | Loss 0.4345 | Accuracy 1.0000 | Val Accuracy 0.7520\n",
      "Epoch 00075 | Loss 0.4589 | Accuracy 0.9667 | Val Accuracy 0.7520\n",
      "Epoch 00076 | Loss 0.4316 | Accuracy 1.0000 | Val Accuracy 0.7540\n",
      "Epoch 00077 | Loss 0.4213 | Accuracy 0.9667 | Val Accuracy 0.7560\n",
      "Epoch 00078 | Loss 0.3975 | Accuracy 0.9833 | Val Accuracy 0.7520\n",
      "Epoch 00079 | Loss 0.3959 | Accuracy 0.9833 | Val Accuracy 0.7560\n",
      "Epoch 00080 | Loss 0.3854 | Accuracy 0.9667 | Val Accuracy 0.7560\n",
      "Epoch 00081 | Loss 0.3725 | Accuracy 0.9833 | Val Accuracy 0.7560\n",
      "Epoch 00082 | Loss 0.3747 | Accuracy 0.9833 | Val Accuracy 0.7540\n",
      "Epoch 00083 | Loss 0.3676 | Accuracy 0.9833 | Val Accuracy 0.7560\n",
      "Epoch 00084 | Loss 0.3503 | Accuracy 1.0000 | Val Accuracy 0.7540\n",
      "Epoch 00085 | Loss 0.3512 | Accuracy 0.9833 | Val Accuracy 0.7540\n",
      "Epoch 00086 | Loss 0.3509 | Accuracy 0.9667 | Val Accuracy 0.7560\n",
      "Epoch 00087 | Loss 0.3515 | Accuracy 1.0000 | Val Accuracy 0.7560\n",
      "Epoch 00088 | Loss 0.3365 | Accuracy 1.0000 | Val Accuracy 0.7540\n",
      "Epoch 00089 | Loss 0.3285 | Accuracy 0.9667 | Val Accuracy 0.7520\n",
      "Epoch 00090 | Loss 0.3119 | Accuracy 1.0000 | Val Accuracy 0.7600\n",
      "Epoch 00091 | Loss 0.2878 | Accuracy 0.9833 | Val Accuracy 0.7560\n",
      "Epoch 00092 | Loss 0.3072 | Accuracy 0.9833 | Val Accuracy 0.7560\n",
      "Epoch 00093 | Loss 0.2868 | Accuracy 1.0000 | Val Accuracy 0.7540\n",
      "Epoch 00094 | Loss 0.2862 | Accuracy 1.0000 | Val Accuracy 0.7520\n",
      "Epoch 00095 | Loss 0.2634 | Accuracy 1.0000 | Val Accuracy 0.7520\n",
      "Epoch 00096 | Loss 0.2627 | Accuracy 1.0000 | Val Accuracy 0.7500\n",
      "Epoch 00097 | Loss 0.2649 | Accuracy 0.9833 | Val Accuracy 0.7520\n",
      "Epoch 00098 | Loss 0.2708 | Accuracy 1.0000 | Val Accuracy 0.7500\n",
      "Epoch 00099 | Loss 0.2702 | Accuracy 0.9833 | Val Accuracy 0.7520\n",
      "Epoch 00100 | Loss 0.2553 | Accuracy 0.9833 | Val Accuracy 0.7540\n",
      "Epoch 00101 | Loss 0.2647 | Accuracy 0.9833 | Val Accuracy 0.7540\n",
      "Epoch 00102 | Loss 0.2469 | Accuracy 1.0000 | Val Accuracy 0.7520\n",
      "Epoch 00103 | Loss 0.2525 | Accuracy 1.0000 | Val Accuracy 0.7520\n",
      "Epoch 00104 | Loss 0.2280 | Accuracy 1.0000 | Val Accuracy 0.7540\n",
      "Epoch 00105 | Loss 0.2159 | Accuracy 1.0000 | Val Accuracy 0.7520\n",
      "Epoch 00106 | Loss 0.2131 | Accuracy 1.0000 | Val Accuracy 0.7520\n",
      "Epoch 00107 | Loss 0.2190 | Accuracy 1.0000 | Val Accuracy 0.7520\n",
      "Epoch 00108 | Loss 0.2112 | Accuracy 1.0000 | Val Accuracy 0.7560\n",
      "Epoch 00109 | Loss 0.2038 | Accuracy 0.9833 | Val Accuracy 0.7540\n",
      "Epoch 00110 | Loss 0.2132 | Accuracy 1.0000 | Val Accuracy 0.7560\n",
      "Epoch 00111 | Loss 0.2167 | Accuracy 0.9833 | Val Accuracy 0.7580\n",
      "Epoch 00112 | Loss 0.2103 | Accuracy 1.0000 | Val Accuracy 0.7580\n",
      "Epoch 00113 | Loss 0.2020 | Accuracy 0.9833 | Val Accuracy 0.7560\n",
      "Epoch 00114 | Loss 0.1870 | Accuracy 1.0000 | Val Accuracy 0.7640\n",
      "Epoch 00115 | Loss 0.1776 | Accuracy 1.0000 | Val Accuracy 0.7600\n",
      "Epoch 00116 | Loss 0.1788 | Accuracy 1.0000 | Val Accuracy 0.7620\n",
      "Epoch 00117 | Loss 0.1817 | Accuracy 0.9833 | Val Accuracy 0.7660\n",
      "Epoch 00118 | Loss 0.1721 | Accuracy 1.0000 | Val Accuracy 0.7600\n",
      "Epoch 00119 | Loss 0.1801 | Accuracy 0.9833 | Val Accuracy 0.7640\n",
      "Epoch 00120 | Loss 0.1649 | Accuracy 1.0000 | Val Accuracy 0.7620\n",
      "Epoch 00121 | Loss 0.1570 | Accuracy 1.0000 | Val Accuracy 0.7620\n",
      "Epoch 00122 | Loss 0.1777 | Accuracy 0.9833 | Val Accuracy 0.7640\n",
      "Epoch 00123 | Loss 0.1626 | Accuracy 1.0000 | Val Accuracy 0.7620\n",
      "Epoch 00124 | Loss 0.1571 | Accuracy 1.0000 | Val Accuracy 0.7560\n",
      "Epoch 00125 | Loss 0.1523 | Accuracy 1.0000 | Val Accuracy 0.7560\n",
      "Epoch 00126 | Loss 0.1534 | Accuracy 1.0000 | Val Accuracy 0.7540\n",
      "Epoch 00127 | Loss 0.1530 | Accuracy 1.0000 | Val Accuracy 0.7620\n",
      "Epoch 00128 | Loss 0.1514 | Accuracy 1.0000 | Val Accuracy 0.7540\n",
      "Epoch 00129 | Loss 0.1472 | Accuracy 1.0000 | Val Accuracy 0.7580\n",
      "Epoch 00130 | Loss 0.1466 | Accuracy 1.0000 | Val Accuracy 0.7620\n",
      "Epoch 00131 | Loss 0.1457 | Accuracy 1.0000 | Val Accuracy 0.7640\n",
      "Epoch 00132 | Loss 0.1316 | Accuracy 1.0000 | Val Accuracy 0.7660\n",
      "Epoch 00133 | Loss 0.1381 | Accuracy 1.0000 | Val Accuracy 0.7620\n",
      "Epoch 00134 | Loss 0.1323 | Accuracy 1.0000 | Val Accuracy 0.7680\n",
      "Epoch 00135 | Loss 0.1353 | Accuracy 1.0000 | Val Accuracy 0.7600\n",
      "Epoch 00136 | Loss 0.1254 | Accuracy 1.0000 | Val Accuracy 0.7600\n",
      "Epoch 00137 | Loss 0.1334 | Accuracy 1.0000 | Val Accuracy 0.7640\n",
      "Epoch 00138 | Loss 0.1177 | Accuracy 1.0000 | Val Accuracy 0.7620\n",
      "Epoch 00139 | Loss 0.1321 | Accuracy 1.0000 | Val Accuracy 0.7620\n",
      "Epoch 00140 | Loss 0.1191 | Accuracy 1.0000 | Val Accuracy 0.7600\n",
      "Epoch 00141 | Loss 0.1019 | Accuracy 1.0000 | Val Accuracy 0.7680\n",
      "Epoch 00142 | Loss 0.1137 | Accuracy 1.0000 | Val Accuracy 0.7700\n",
      "Epoch 00143 | Loss 0.1165 | Accuracy 1.0000 | Val Accuracy 0.7720\n",
      "Epoch 00144 | Loss 0.1185 | Accuracy 1.0000 | Val Accuracy 0.7680\n",
      "Epoch 00145 | Loss 0.1004 | Accuracy 1.0000 | Val Accuracy 0.7680\n",
      "Epoch 00146 | Loss 0.1070 | Accuracy 1.0000 | Val Accuracy 0.7700\n",
      "Epoch 00147 | Loss 0.1120 | Accuracy 1.0000 | Val Accuracy 0.7700\n",
      "Epoch 00148 | Loss 0.1027 | Accuracy 1.0000 | Val Accuracy 0.7640\n",
      "Epoch 00149 | Loss 0.1078 | Accuracy 1.0000 | Val Accuracy 0.7700\n",
      "Testing Accuracy: 0.7599999904632568\n"
     ]
    }
   ],
   "source": [
    "# Training hyperparameters\n",
    "weight_decay = 5e-4\n",
    "n_epochs = 150\n",
    "lr = 1e-3\n",
    "\n",
    "# create the Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "\n",
    "Train(model, train_sampler, val_sampler, optimizer, n_epochs)\n",
    "Test(model, test_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624d656d",
   "metadata": {},
   "source": [
    "Generally, the results should be very similar to the previous one on this dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp4332",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
