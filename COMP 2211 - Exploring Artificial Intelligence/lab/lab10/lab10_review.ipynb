{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **COMP 2211 Exploring Artificial Intelligence** #\n","## Lab 10 Reinforcement Learning Review ##\n","\n","<img src=\"https://miro.medium.com/max/4800/1*7PoZafFLEVXQiseVx5y4cw.jpeg\" width=\"600\" height=\"326\" /> \n"],"metadata":{"id":"PdWYAjIFhjBr"}},{"cell_type":"markdown","source":["# Backgroud knowledge"],"metadata":{"id":"pF6UZj4Y1Tmv"}},{"cell_type":"markdown","source":["## In Class Knowledge"],"metadata":{"id":"xwPwr9R87pRu"}},{"cell_type":"markdown","source":["### Discount\n","\n","**Discounting** is a concept, where a parameter called the **discount factor**, $\\gamma \\in [0,1)$, and $0 \\leq \\gamma \\leq 1$. It is a power to multiply a reward.\n","\n","**Discounted sum of future rewards** $=reward_{now}+\\sum_{i=1}^{\\infty} \\gamma^i \\times reward_i$\n","\n","**Bellman equation** $V(s)=R(s,a)+γ*\\sum P(s'|s,a)*V(s')$.\n","1.   $V(s)$: the value of state $s$.\n","2.   $R(s, a)$: the immediate reward for taking action a in state $s$.\n","3.   $P(s'|s, a)$: the probability of transitioning to state $s'$ given state s and action $a$.\n","4.   $V(s')$: the value of the next state $s'$.\n","\n","\n"],"metadata":{"id":"kNKyJRua1ZI_"}},{"cell_type":"markdown","source":["### Markov State\n","\n","$S_t\\ is\\ Markov \\iff P(S_{t+1}|S_t)=P(S_{t+1}|S_1,S_2, \\dots, S_t)$\n","\n","1. The future is independent of the past, given the present.\n","2. The present captures information about the past.\n","3. Once the present is known, the history may be thrown away.\n","\n"],"metadata":{"id":"4baenIZOD6CM"}},{"cell_type":"markdown","source":["### A Markov System with Rewards\n","\n","- Components\n","    - MRP: $(S, T, R, γ)$\n","    - A set of N states $s_i$; each $s_i$ has a reward $r_i$ [$\\vec r$]\n","    - A transition **probability** matrix [$T$]\n","        - where $T_{i,j}=P[i \\to j]=P[next= s_j|now=s_i]$\n","        - each row sums up to be 1\n","    - Discount factor $\\gamma \\in (0,1)$ to all rewards to compute the expected discounted sum of future rewards **starting in state $s_i$** [$\\vec v$]:\n","\n","- Directly solve the linear equation\n","\n","$$\n","\\vec v=\\vec r+\\gamma T\\vec v \\to \\vec v=(I-\\gamma T)^{-1} \\vec r\n","$$\n","\n","- Dynamic Programming (we will do Value Iteration)\n","    - $\\vec v^{(0)}=\\vec r$\n","    - $\\vec v^{(k)}=\\vec r+\\gamma T\\vec v^{(k-1)}$\n","    - Compute the expected discounted sum of rewards over the next k time steps from now\n","    - Stop when the maximum absolute difference between two successive expected discounted sum of rewards is less than a threshold ($\\max_i |\\vec v^{(k)}-\\vec v^{(k-1)}|<\\xi$)\n","- Monte-Carlo evaluation\n","- Temporal-Difference learning\n","\n","\n","### Example: Weather\n","\n","- We will use the Markov chain on [the lecture slide](https://course.cse.ust.hk/comp2211/notes/12-reinforcement-learning-full.pdf) P23-28\n"],"metadata":{"id":"ZMIwUWX_EX_T"}},{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"pK-qb4wYGUDy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# T: transition probability matrix\n","#    sun wind hail\n","T = np.array([[0.5, 0.5, 0.0],  # sun\n","        [0.5, 0.0, 0.5],  # wind\n","        [0.0, 0.5, 0.5]])  # hail\n","\n","# R: the reward for each state\n","#       sun wind hail\n","R = np.array([4, 0, -8], dtype=float)\n","\n","# gamma: the discount value\n","gamma = 0.5\n","\n","\n","# Try to directly solve this equation:\n","def cal_reward_directly(T, R, gamma):\n","  return np.linalg.inv(np.identity(T.shape[0]) - gamma * T) @ R\n","\n","print(\"Total future rewards calculated directly:\")\n","print(cal_reward_directly(T, R, gamma))\n","\n","\n","# Now we use iterative method to find the total reward:\n","# threshold: the difference we accept bewteen 2 successive run\n","threshold = 1e-1\n","\n","def cal_reward_by_iterations(T, R, gamma, threshold):\n","  reward_previous_iteration = np.zeros_like(R, dtype=float)\n","  reward_this_iteration = R\n","  count = 0           # Record the time of iterations required to reach the result\n","  while np.max(np.abs(reward_previous_iteration - reward_this_iteration)) > threshold:\n","    count += 1\n","    reward_previous_iteration = reward_this_iteration\n","    reward_this_iteration = R + gamma * (T @ reward_this_iteration)\n","\n","  print(f\"It took {count} iterations to find the total reward with the threshold = {threshold}.\")\n","  return reward_this_iteration\n","\n","print(\"\\n\\nTotal future rewards calculated by iteration:\")\n","print(cal_reward_by_iterations(T, R, gamma, threshold))\n","print(\"Try to change the threshold value to see how it affects the number of iterations required and the total reward value\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GYof_OXSDyRn","outputId":"649b5986-201b-402e-c7f8-c983bb2d037a","executionInfo":{"status":"ok","timestamp":1683128170534,"user_tz":-480,"elapsed":2,"user":{"displayName":"Jialiang Wang","userId":"01116727958111139047"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total future rewards calculated directly:\n","[  4.8  -1.6 -11.2]\n","\n","\n","Total future rewards calculated by iteration:\n","It took 5 iterations to find the total reward with the threshold = 0.1.\n","[  4.83984375  -1.55859375 -11.15625   ]\n","Try to change the threshold value to see how it affects the number of iterations required and the total reward value\n"]}]},{"cell_type":"markdown","source":["### Markov Decision Process(MDP)\n","\n","- MDP: $(S, A, T, R, γ)$\n","- A Markov System with Rewards and **Actions/Decisions** [a]\n","- Each action a has a matrix $T^{(a)}$, where $T_{i,j}^{(a)}=P[i \\overset{a}{ \\to } j]=P[next=s_j|now=s_i, action=a]$\n","- Relax reward from a vector to be a matrix R, where $r_{i,j}$ is the reward from $s_i$ to $s_j$\n","- **Bellman Optimality Equation**\n","    - $v^{(k)}_i=\\max_a \\{ \\sum_j T^{(a)} _{i,j} (r _{i,j} + \\gamma  v^{(k-1)} _j)  \\}$\n","    - $\\vec v^{(k)}=\\max_a \\{ [T^{(a)} \\otimes R]+ \\gamma T^{(a)} \\vec v^{(k-1)} \\}$ (elementwise multiplication $\\otimes$→ row sum $[\\cdot ]$ → elementwise max)\n","    - Equivalently, $\\vec v^{(k)}=\\max_a \\{ R^{(a)} + \\gamma T^{(a)} \\vec v^{(k-1)} \\}$\n","    - Corner case: a **terminal state** can be represented as a state that transitions back into itself and yields reward 0 with probability 1, regardless of the action taken. **You need to handle these states carefully.**\n","- A **policy** is a mapping from states to actions\n","- Value Iteration\n","    - **Near optimal policy** $\\vec v^{(k)}=\\text{argmax}_a \\{ [T^{(a)} \\otimes R]+ \\gamma T^{(a)} \\vec v^{(k-1)} \\}$ (elementwise multiplication $\\otimes$, row sum $[\\cdot ]$, elementwise argmax)\n","    - Stop until $\\max_i |\\vec v^{(k)}-\\vec v^{(k-1)}|<\\xi$"],"metadata":{"id":"U0ygGma8DuJf"}},{"cell_type":"markdown","source":["## High Level Summary"],"metadata":{"id":"aQuO3eI377bf"}},{"cell_type":"markdown","source":["> Consider the game of chess. The only real reward signal comes at the end of the game when we either win, earning a reward of, say, 1, or when we lose, receiving a reward of, say, -1. **So reinforcement learners must deal with the *credit assignment* problem: determining which actions to credit or blame for an outcome.** The same goes for an employee who gets a promotion on October 11. That promotion likely reflects a large number of well-chosen actions over the previous year. **Getting more promotions in the future requires figuring out what actions along the way led to the promotion.**\n","> \n",">Reinforcement learners may also have to deal with the problem of **partial observability**. That is, the current observation might not tell you everything about your current state. Say a cleaning robot found itself trapped in one of many identical closets in a house. Inferring the precise location of the robot might require considering its previous observations before entering the closet.\n",">\n",">Finally, at any given point, reinforcement learners might know of one good policy, but there might be many other better policies that the agent has never tried. The reinforcement learner must constantly choose whether to ***exploit*** the best (currently) known strategy as a policy, or to ***explore*** the space of strategies, potentially giving up some short-run reward in exchange for knowledge.\n",">\n",">When the environment is fully observed, we call the reinforcement learning problem a ***Markov decision process***. When the state does not depend on the previous actions, we call the problem a ***contextual bandit problem***. When there is no state, just a set of available actions with initially unknown rewards, this problem is the classic ***multi-armed bandit problem***.\n","\n","(From [d2l.ai](https://d2l.ai/chapter_introduction/index.html?highlight=reinforcement#reinforcement-learning))"],"metadata":{"id":"suQ4_NuK7-gm"}},{"cell_type":"markdown","source":["# Credit\n","1. the image: [medium](https://medium.datadriveninvestor.com/alphago-a-documentary-about-artificial-intelligence-37c147252889)"],"metadata":{"id":"DPm_G6kI9Phw"}}]}