{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **COMP 2211 Exploring Artificial Intelligence** #\n","## Lab 10 Reinforcement Learning ##\n","\n","<img src=\"https://miro.medium.com/max/4800/1*7PoZafFLEVXQiseVx5y4cw.jpeg\" width=\"600\" height=\"326\" /> "],"metadata":{"id":"PdWYAjIFhjBr"}},{"cell_type":"markdown","source":["# Introduction\n","MDP (Markov Decision Process) is a good starting point for studying Reinforcement Learning.\n","\n","In this lab, let's build up an MDP system.\n"],"metadata":{"id":"mjWtCwC_JaLQ"}},{"cell_type":"markdown","source":["# Problem Setting\n","You wake up in your bed.\n","\n","You're in Hong Kong now.\n","\n","You know you have finished all your final exams. \n","Today is your first day of the trip to Hong Kong during the summer break. \n","Now you want to plan for your activities sequence for today. \n","\n","Let's use an integer to represent the Hong Kong Dollar you have.\n","To start with, you have **m** HKD. \n","You will choose one of two activities (if you only have 1 HKD, you can't choose activity B) repeatedly until you double the money you have (have at least 2m HKD) or spend it all (have 0 HKD).\n","\n","1. Activity A costs 1 HKD. With probability 0.05, it will return 2 HKD, and 0 HKD otherwise.\n","\n","2. Activity B costs 2 HKD. With probability 0.03, it will return 4 HKD, and 0 HKD otherwise.\n","\n","Of course, you hope to take the best strategy to maximize your likelihood of ending up with at least **2m** HKD."],"metadata":{"id":"SvKzQ3O9paow"}},{"cell_type":"markdown","source":["# Task\n","Now we want to set up an MDP system. Using the Bellman Optimality Equation, with $\\xi=10^{-5}$, we can obtain the near-optimal policy through the Value Iteration process. \n","\n","Input: an integer $m \\le 100$, a real number $\\gamma \\in (0,1)$ standing for the discount factor.\n","\n","To better test your implementation, we provide a constraint to ensure the uniqueness of **correct implementation**: Lump sum reward (one-off). Reward **0** for ending with **no money**, and reward **$10^5$** for ending with at least **2m** HKD.\n","\n","Hint 1: for the Lump sum reward, it doesn't appreciate whether you have almost won (e.g., having 2m-1 HKD) or not. You only care about the final result of your day.\n","\n","Hint 2: feel free to check out the review part of this lab.\n","\n","Hint 3: discount is also a way to encourage faster winning."],"metadata":{"id":"IiZoCUuGI-AR"}},{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"781Tce3XZtz4"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v5q59hTFhanA"},"outputs":[],"source":["def construction(m):\n","    # Return a tuple R, T\n","    # R: numpy array of shape = (2m+1, 2m+1), dtype = float\n","    # T: numpy array of shape = (2, 2m+1, 2m+1), dtype = float\n","\n","    # Setting of the two activities\n","    pr = 0.05, 0.03                 # Tuple type\n","    cost = 1, 2\n","    reward = 2, 4\n","    #################################################################################\n","    # SRART OF YOUR CODE                            #\n","    # TODO:                                    #\n","    # Construct reward vecter R;                        #\n","    # Construct transition matrix T1, T2 standing for two actions in T. #\n","    #################################################################################\n","    R = None \n","    T = None, None\n","\n","\n","    #################################################################################\n","    # END OF YOUR CODE                             #\n","    #################################################################################\n","\n","    return R, T\n","\n","def valueIteration(m, gamma, threshold = 1e-5):\n","    # Return a numpy array optimal_policy with shape = (2m+1, ), dtype = int\n","    # Optimal_policy consists of 0 or 1, meaning firstly starting action/activity 1 or 2 respectively\n","    # Optimal_policy[0] and optimal_policy[2m+1] are 0\n","\n","    R, T = construction(m)\n","    #######################################################################\n","    # SRART OF YOUR CODE                       #\n","    # TODO:                               #\n","    # Initilize expected discounted sum of rewards vector v_prev #\n","    #######################################################################\n","    v_prev = None                   # shape = (2m+1,)\n","    \n","    \n","    #######################################################################\n","    # END OF YOUR CODE                        #\n","    #######################################################################\n","    \n","    optimal_policy = None \n","    while True:\n","        ###################################################################\n","        # SRART OF YOUR CODE                     #\n","        # TODO:                             #\n","        # Complete the computation of v_now and optimal_policy.  #\n","        ###################################################################\n","        v_now = None\n","\n","        \n","        \n","        ###################################################################\n","        # END OF YOUR CODE                      #\n","        ###################################################################\n","\n","        error = np.abs(v_now - v_prev).max()\n","        if error < threshold:\n","          break\n","        v_prev = v_now\n","    \n","    print('valueIteration function with m =', str(m), 'and gamma =', str(gamma))\n","    print('Your v_now:\\n', v_now)             # Variable scope = this function\n","    print('Your optimal_policy:', optimal_policy)\n","    print('\\n')\n","    return optimal_policy"]},{"cell_type":"markdown","source":["# Test\n","\n","Feel free to try your own parameters after finish these tests."],"metadata":{"id":"iA0fosDIcSdO"}},{"cell_type":"code","source":["R, T = construction(2)\n","print('Your R:')\n","print(R)\n","print('Your T:')\n","print(T)\n","\n","# np.array(...) is the expected resulto0, 0, 0, 0, 100000], [0, 0, 0, 0, 100000], [0, 0, 0, 0, 100000], [0, 0, 0, 0, 100000], [0, 0, 0, 0, 0]])).all()\n","assert (T == np.array([[[1, 0, 0, 0, 0], [0.95, 0, 0.05, 0, 0], [0, 0.95, 0, 0.05, 0], [0, 0, 0.95, 0, 0.05], [0, 0, 0, 0, 1]], [[1, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0.97, 0, 0, 0, 0.03], [0, 0.97, 0, 0, 0.03], [0, 0, 0, 0, 1]]])).all()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6f937A4_Sg2s","executionInfo":{"status":"ok","timestamp":1683129057815,"user_tz":-480,"elapsed":3,"user":{"displayName":"Jialiang Wang","userId":"01116727958111139047"}},"outputId":"4a5ff440-b3df-4b6f-da59-27a9ff9cccd6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Your R:\n","[[     0.      0.      0.      0. 100000.]\n"," [     0.      0.      0.      0. 100000.]\n"," [     0.      0.      0.      0. 100000.]\n"," [     0.      0.      0.      0. 100000.]\n"," [     0.      0.      0.      0.      0.]]\n","Your T:\n","[[[1.   0.   0.   0.   0.  ]\n","  [0.95 0.   0.05 0.   0.  ]\n","  [0.   0.95 0.   0.05 0.  ]\n","  [0.   0.   0.95 0.   0.05]\n","  [0.   0.   0.   0.   1.  ]]\n","\n"," [[1.   0.   0.   0.   0.  ]\n","  [0.   0.   0.   0.   0.  ]\n","  [0.97 0.   0.   0.   0.03]\n","  [0.   0.97 0.   0.   0.03]\n","  [0.   0.   0.   0.   1.  ]]]\n"]}]},{"cell_type":"code","source":["# np.array(...) is the expected result\n","assert (valueIteration(m=2, gamma=1) == np.array([0, 0, 1, 0, 0])).all()\n","assert (valueIteration(m=4, gamma=1) == np.array([0, 0, 1, 1, 1, 0, 1, 0, 0])).all()\n","assert (valueIteration(m=4, gamma=0.9) == np.array([0, 0, 1, 0, 1, 0, 1, 0, 0])).all()\n","assert (valueIteration(m=10, gamma=1) == np.array([0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0])).all()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QcGL7rj52FUd","outputId":"d716460a-cfbf-4c80-cfb3-2e8eec9c86d3","executionInfo":{"status":"ok","timestamp":1683129057815,"user_tz":-480,"elapsed":3,"user":{"displayName":"Jialiang Wang","userId":"01116727958111139047"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["valueIteration function with m = 2 and gamma = 1\n","Your v_now:\n"," [   0.  150. 3000. 7850.    0.]\n","Your optimal_policy: [0 0 1 0 0]\n","\n","\n","valueIteration function with m = 4 and gamma = 1\n","Your v_now:\n"," [0.00000000e+00 1.43342530e-01 2.86685070e+00 7.50159258e+00\n"," 9.55616902e+01 2.45418347e+02 3.09269484e+03 7.93806009e+03\n"," 0.00000000e+00]\n","Your optimal_policy: [0 0 1 1 1 0 1 0 0]\n","\n","\n","valueIteration function with m = 4 and gamma = 0.9\n","Your v_now:\n"," [0.00000000e+00 1.03284014e-01 2.29520033e+00 5.78773015e+00\n"," 8.50074197e+01 2.11020860e+02 3.07421148e+03 7.62845081e+03\n"," 0.00000000e+00]\n","Your optimal_policy: [0 0 1 0 1 0 1 0 0]\n","\n","\n","valueIteration function with m = 10 and gamma = 1\n","Your v_now:\n"," [0.00000000e+00 1.25384912e-10 2.50885562e-09 6.56373936e-09\n"," 8.36285207e-08 2.14773850e-07 2.70664665e-06 6.94708666e-06\n"," 8.75175661e-05 2.24628733e-04 2.82974625e-03 7.26301088e-03\n"," 9.14951404e-02 2.34837536e-01 2.95834322e+00 7.59308080e+00\n"," 9.56530978e+01 2.45509618e+02 3.09278350e+03 7.93814433e+03\n"," 0.00000000e+00]\n","Your optimal_policy: [0 0 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0]\n","\n","\n"]}]},{"cell_type":"markdown","source":["# What Next?\n","1. You may want to check out the course [CS234 Reinforcement Learning provided by Stanford](https://web.stanford.edu/class/cs234) (one of the homework is also the ideal source for this lab).\n","\n","2. [Gymnasium](https://gymnasium.farama.org/) is a standard API for reinforcement learning and a diverse collection of reference environments\n","\n","3. In our setting, the transition matrix is very sparse. In this case, vectorization and matrix multiplication are not that beneficial.\n"],"metadata":{"id":"MJXm7mCeXgpH"}}]}